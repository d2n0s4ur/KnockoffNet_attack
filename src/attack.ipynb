{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KnockoffNet Attack on CIFAR10\n",
    "# import packages\n",
    "import torch\n",
    "import timm\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "batch_size = 1000\n",
    "epochs = 100 # 1K X 100 = 100K\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 60000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset loader\n",
    "data_path = '../data/'\n",
    "\n",
    "train_data_cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data_cifar10 = datasets.CIFAR10(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "train_data_mnist = datasets.MNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data_mnist = datasets.MNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "len(train_data_cifar10), len(test_data_cifar10), len(train_data_mnist), len(test_data_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49139968 0.48215841 0.44653091] [0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "# Normalize CIFAR10\n",
    "mean_cifar10 = train_data_cifar10.data.mean(axis=(0,1,2))/255\n",
    "std_cifar10 = train_data_cifar10.data.std(axis=(0,1,2))/255\n",
    "\n",
    "train_data_cifar10.transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10, std_cifar10)\n",
    "])\n",
    "\n",
    "test_data_cifar10.transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10, std_cifar10)\n",
    "])\n",
    "\n",
    "train_loader_cifar10 = torch.utils.data.DataLoader(train_data_cifar10, batch_size=batch_size, shuffle=True)\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(test_data_cifar10, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print (mean_cifar10, std_cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "          Identity-4           [-1, 64, 32, 32]               0\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "          Identity-7           [-1, 64, 32, 32]               0\n",
      "              ReLU-8           [-1, 64, 32, 32]               0\n",
      "          Identity-9           [-1, 64, 32, 32]               0\n",
      "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "             ReLU-12           [-1, 64, 32, 32]               0\n",
      "       BasicBlock-13           [-1, 64, 32, 32]               0\n",
      "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
      "         Identity-16           [-1, 64, 32, 32]               0\n",
      "             ReLU-17           [-1, 64, 32, 32]               0\n",
      "         Identity-18           [-1, 64, 32, 32]               0\n",
      "           Conv2d-19           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
      "             ReLU-21           [-1, 64, 32, 32]               0\n",
      "       BasicBlock-22           [-1, 64, 32, 32]               0\n",
      "           Conv2d-23          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-24          [-1, 128, 16, 16]             256\n",
      "         Identity-25          [-1, 128, 16, 16]               0\n",
      "             ReLU-26          [-1, 128, 16, 16]               0\n",
      "         Identity-27          [-1, 128, 16, 16]               0\n",
      "           Conv2d-28          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 16, 16]             256\n",
      "           Conv2d-30          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-31          [-1, 128, 16, 16]             256\n",
      "             ReLU-32          [-1, 128, 16, 16]               0\n",
      "       BasicBlock-33          [-1, 128, 16, 16]               0\n",
      "           Conv2d-34          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-35          [-1, 128, 16, 16]             256\n",
      "         Identity-36          [-1, 128, 16, 16]               0\n",
      "             ReLU-37          [-1, 128, 16, 16]               0\n",
      "         Identity-38          [-1, 128, 16, 16]               0\n",
      "           Conv2d-39          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-40          [-1, 128, 16, 16]             256\n",
      "             ReLU-41          [-1, 128, 16, 16]               0\n",
      "       BasicBlock-42          [-1, 128, 16, 16]               0\n",
      "           Conv2d-43            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-44            [-1, 256, 8, 8]             512\n",
      "         Identity-45            [-1, 256, 8, 8]               0\n",
      "             ReLU-46            [-1, 256, 8, 8]               0\n",
      "         Identity-47            [-1, 256, 8, 8]               0\n",
      "           Conv2d-48            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-49            [-1, 256, 8, 8]             512\n",
      "           Conv2d-50            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-51            [-1, 256, 8, 8]             512\n",
      "             ReLU-52            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-53            [-1, 256, 8, 8]               0\n",
      "           Conv2d-54            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-55            [-1, 256, 8, 8]             512\n",
      "         Identity-56            [-1, 256, 8, 8]               0\n",
      "             ReLU-57            [-1, 256, 8, 8]               0\n",
      "         Identity-58            [-1, 256, 8, 8]               0\n",
      "           Conv2d-59            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-60            [-1, 256, 8, 8]             512\n",
      "             ReLU-61            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-62            [-1, 256, 8, 8]               0\n",
      "           Conv2d-63            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-64            [-1, 512, 4, 4]           1,024\n",
      "         Identity-65            [-1, 512, 4, 4]               0\n",
      "             ReLU-66            [-1, 512, 4, 4]               0\n",
      "         Identity-67            [-1, 512, 4, 4]               0\n",
      "           Conv2d-68            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-69            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-70            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-71            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-72            [-1, 512, 4, 4]               0\n",
      "       BasicBlock-73            [-1, 512, 4, 4]               0\n",
      "           Conv2d-74            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-75            [-1, 512, 4, 4]           1,024\n",
      "         Identity-76            [-1, 512, 4, 4]               0\n",
      "             ReLU-77            [-1, 512, 4, 4]               0\n",
      "         Identity-78            [-1, 512, 4, 4]               0\n",
      "           Conv2d-79            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-80            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-81            [-1, 512, 4, 4]               0\n",
      "       BasicBlock-82            [-1, 512, 4, 4]               0\n",
      "AdaptiveAvgPool2d-83            [-1, 512, 1, 1]               0\n",
      "          Flatten-84                  [-1, 512]               0\n",
      "SelectAdaptivePool2d-85                  [-1, 512]               0\n",
      "           Linear-86                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,173,962\n",
      "Trainable params: 11,173,962\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 19.76\n",
      "Params size (MB): 42.63\n",
      "Estimated Total Size (MB): 62.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#load pretrained model(vitcim model)\n",
    "victim_model = timm.create_model(\"resnet18\", pretrained=False)\n",
    "\n",
    "# override model\n",
    "victim_model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "victim_model.maxpool = nn.Identity()  # type: ignore\n",
    "victim_model.fc = nn.Linear(512,  10)\n",
    "\n",
    "victim_model.load_state_dict(\n",
    "            torch.hub.load_state_dict_from_url(\n",
    "                      \"https://huggingface.co/edadaltocg/resnet18_cifar10/resolve/main/pytorch_model.bin\",\n",
    "                       map_location=\"cuda\", \n",
    "                       file_name=\"resnet18_cifar10.pth\",\n",
    "             )\n",
    ")\n",
    "\n",
    "victim_model.to(device)\n",
    "summary(victim_model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,181,642\n",
      "Trainable params: 11,181,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 43.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# define attacker model\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정합니다.\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n",
    "        )\n",
    "\n",
    "        # identity mapping, input과 output의 feature map size, filter 수가 동일한 경우 사용.\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # projection mapping using 1x1conv\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_block, num_classes=10, init_weights=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels=64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # weights inittialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        x = self.conv3_x(output)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # define weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def resnet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "attacker_model = resnet18().to(device)\n",
    "\n",
    "summary(attacker_model, (3, 32, 32), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_victim_softmax(query):\n",
    "    \"\"\"\n",
    "    This function takes a query and returns the softmax output of the victim model.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = victim_model(query)\n",
    "        softmax = torch.nn.functional.softmax(output, dim=1)\n",
    "    return softmax\n",
    "\n",
    "def query_to_victim_onehot(query):\n",
    "    \"\"\"\n",
    "    This function takes a query and returns the one-hot output of the victim model.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = victim_model(query)\n",
    "        onehot = torch.nn.functional.one_hot(torch.argmax(output, dim=1), num_classes=10)\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax output:  tensor([[0.1055, 0.0744, 0.1003, 0.1389, 0.0987, 0.1238, 0.0945, 0.0932, 0.0902,\n",
      "         0.0804]], device='cuda:0')\n",
      "One-hot output:  tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test query\n",
    "query = torch.rand(1, 3, 32, 32).to(device)\n",
    "softmax = query_to_victim_softmax(query)\n",
    "onehot = query_to_victim_onehot(query)\n",
    "\n",
    "print(\"Softmax output: \", softmax)\n",
    "print(\"One-hot output: \", onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Query: 10000, loss: 2.280\n",
      "query: 10000, Accuracy: 10.910 %\n",
      "Epoch: 1, Query: 20000, loss: 2.292\n",
      "query: 20000, Accuracy: 14.080 %\n",
      "Epoch: 1, Query: 30000, loss: 2.279\n",
      "query: 30000, Accuracy: 16.190 %\n",
      "Epoch: 1, Query: 40000, loss: 2.260\n",
      "query: 40000, Accuracy: 18.410 %\n",
      "Epoch: 1, Query: 50000, loss: 2.238\n",
      "query: 50000, Accuracy: 19.980 %\n",
      "Epoch: 2, Query: 60000, loss: 2.208\n",
      "query: 60000, Accuracy: 21.980 %\n",
      "Epoch: 2, Query: 70000, loss: 2.199\n",
      "query: 70000, Accuracy: 22.180 %\n",
      "Epoch: 2, Query: 80000, loss: 2.213\n",
      "query: 80000, Accuracy: 22.400 %\n",
      "Epoch: 2, Query: 90000, loss: 2.193\n",
      "query: 90000, Accuracy: 22.510 %\n",
      "Epoch: 2, Query: 100000, loss: 2.180\n",
      "query: 100000, Accuracy: 22.750 %\n",
      "Epoch: 3, Query: 110000, loss: 2.208\n",
      "query: 110000, Accuracy: 22.790 %\n",
      "Epoch: 3, Query: 120000, loss: 2.193\n",
      "query: 120000, Accuracy: 22.780 %\n",
      "Epoch: 3, Query: 130000, loss: 2.191\n",
      "query: 130000, Accuracy: 22.800 %\n",
      "Epoch: 3, Query: 140000, loss: 2.182\n",
      "query: 140000, Accuracy: 22.830 %\n",
      "Epoch: 3, Query: 150000, loss: 2.184\n",
      "query: 150000, Accuracy: 22.830 %\n",
      "Epoch: 4, Query: 160000, loss: 2.202\n",
      "query: 160000, Accuracy: 22.850 %\n",
      "Epoch: 4, Query: 170000, loss: 2.184\n",
      "query: 170000, Accuracy: 22.840 %\n",
      "Epoch: 4, Query: 180000, loss: 2.177\n",
      "query: 180000, Accuracy: 22.820 %\n",
      "Epoch: 4, Query: 190000, loss: 2.211\n",
      "query: 190000, Accuracy: 22.820 %\n",
      "Epoch: 4, Query: 200000, loss: 2.190\n",
      "query: 200000, Accuracy: 22.810 %\n",
      "Epoch: 5, Query: 210000, loss: 2.200\n",
      "query: 210000, Accuracy: 22.790 %\n",
      "Epoch: 5, Query: 220000, loss: 2.190\n",
      "query: 220000, Accuracy: 22.840 %\n",
      "Epoch: 5, Query: 230000, loss: 2.201\n",
      "query: 230000, Accuracy: 22.840 %\n",
      "Epoch: 5, Query: 240000, loss: 2.185\n",
      "query: 240000, Accuracy: 22.810 %\n",
      "Epoch: 5, Query: 250000, loss: 2.196\n",
      "query: 250000, Accuracy: 22.830 %\n",
      "Epoch: 6, Query: 260000, loss: 2.206\n",
      "query: 260000, Accuracy: 22.840 %\n",
      "Epoch: 6, Query: 270000, loss: 2.199\n",
      "query: 270000, Accuracy: 22.830 %\n",
      "Epoch: 6, Query: 280000, loss: 2.183\n",
      "query: 280000, Accuracy: 22.830 %\n",
      "Epoch: 6, Query: 290000, loss: 2.188\n",
      "query: 290000, Accuracy: 22.820 %\n",
      "Epoch: 6, Query: 300000, loss: 2.186\n",
      "query: 300000, Accuracy: 22.810 %\n",
      "Epoch: 7, Query: 310000, loss: 2.186\n",
      "query: 310000, Accuracy: 22.800 %\n",
      "Epoch: 7, Query: 320000, loss: 2.199\n",
      "query: 320000, Accuracy: 22.820 %\n",
      "Epoch: 7, Query: 330000, loss: 2.202\n",
      "query: 330000, Accuracy: 22.840 %\n",
      "Epoch: 7, Query: 340000, loss: 2.196\n",
      "query: 340000, Accuracy: 22.840 %\n",
      "Epoch: 7, Query: 350000, loss: 2.194\n",
      "query: 350000, Accuracy: 22.830 %\n",
      "Epoch: 8, Query: 360000, loss: 2.179\n",
      "query: 360000, Accuracy: 22.810 %\n",
      "Epoch: 8, Query: 370000, loss: 2.208\n",
      "query: 370000, Accuracy: 22.810 %\n",
      "Epoch: 8, Query: 380000, loss: 2.192\n",
      "query: 380000, Accuracy: 22.810 %\n",
      "Epoch: 8, Query: 390000, loss: 2.194\n",
      "query: 390000, Accuracy: 22.840 %\n",
      "Epoch: 8, Query: 400000, loss: 2.179\n",
      "query: 400000, Accuracy: 22.840 %\n",
      "Epoch: 9, Query: 410000, loss: 2.186\n",
      "query: 410000, Accuracy: 22.800 %\n",
      "Epoch: 9, Query: 420000, loss: 2.187\n",
      "query: 420000, Accuracy: 22.820 %\n",
      "Epoch: 9, Query: 430000, loss: 2.205\n",
      "query: 430000, Accuracy: 22.810 %\n",
      "Epoch: 9, Query: 440000, loss: 2.198\n",
      "query: 440000, Accuracy: 22.800 %\n",
      "Epoch: 9, Query: 450000, loss: 2.196\n",
      "query: 450000, Accuracy: 22.820 %\n",
      "Epoch: 10, Query: 460000, loss: 2.203\n",
      "query: 460000, Accuracy: 22.830 %\n",
      "Epoch: 10, Query: 470000, loss: 2.199\n",
      "query: 470000, Accuracy: 22.800 %\n",
      "Epoch: 10, Query: 480000, loss: 2.189\n",
      "query: 480000, Accuracy: 22.810 %\n",
      "Epoch: 10, Query: 490000, loss: 2.188\n",
      "query: 490000, Accuracy: 22.830 %\n",
      "Epoch: 10, Query: 500000, loss: 2.193\n",
      "query: 500000, Accuracy: 22.850 %\n",
      "Epoch: 11, Query: 510000, loss: 2.193\n",
      "query: 510000, Accuracy: 22.840 %\n",
      "Epoch: 11, Query: 520000, loss: 2.194\n",
      "query: 520000, Accuracy: 22.830 %\n",
      "Epoch: 11, Query: 530000, loss: 2.194\n",
      "query: 530000, Accuracy: 22.820 %\n",
      "Epoch: 11, Query: 540000, loss: 2.204\n",
      "query: 540000, Accuracy: 22.810 %\n",
      "Epoch: 11, Query: 550000, loss: 2.198\n",
      "query: 550000, Accuracy: 22.820 %\n",
      "Epoch: 12, Query: 560000, loss: 2.190\n",
      "query: 560000, Accuracy: 22.820 %\n",
      "Epoch: 12, Query: 570000, loss: 2.199\n",
      "query: 570000, Accuracy: 22.820 %\n",
      "Epoch: 12, Query: 580000, loss: 2.180\n",
      "query: 580000, Accuracy: 22.800 %\n",
      "Epoch: 12, Query: 590000, loss: 2.195\n",
      "query: 590000, Accuracy: 22.810 %\n",
      "Epoch: 12, Query: 600000, loss: 2.189\n",
      "query: 600000, Accuracy: 22.810 %\n",
      "Epoch: 13, Query: 610000, loss: 2.198\n",
      "query: 610000, Accuracy: 22.810 %\n",
      "Epoch: 13, Query: 620000, loss: 2.197\n",
      "query: 620000, Accuracy: 22.830 %\n",
      "Epoch: 13, Query: 630000, loss: 2.204\n",
      "query: 630000, Accuracy: 22.820 %\n",
      "Epoch: 13, Query: 640000, loss: 2.198\n",
      "query: 640000, Accuracy: 22.830 %\n",
      "Epoch: 13, Query: 650000, loss: 2.189\n",
      "query: 650000, Accuracy: 22.830 %\n",
      "Epoch: 14, Query: 660000, loss: 2.194\n",
      "query: 660000, Accuracy: 22.820 %\n",
      "Epoch: 14, Query: 670000, loss: 2.183\n",
      "query: 670000, Accuracy: 22.840 %\n",
      "Epoch: 14, Query: 680000, loss: 2.201\n",
      "query: 680000, Accuracy: 22.810 %\n",
      "Epoch: 14, Query: 690000, loss: 2.195\n",
      "query: 690000, Accuracy: 22.820 %\n",
      "Epoch: 14, Query: 700000, loss: 2.186\n",
      "query: 700000, Accuracy: 22.820 %\n",
      "Epoch: 15, Query: 710000, loss: 2.162\n",
      "query: 710000, Accuracy: 22.820 %\n",
      "Epoch: 15, Query: 720000, loss: 2.209\n",
      "query: 720000, Accuracy: 22.810 %\n",
      "Epoch: 15, Query: 730000, loss: 2.186\n",
      "query: 730000, Accuracy: 22.830 %\n",
      "Epoch: 15, Query: 740000, loss: 2.203\n",
      "query: 740000, Accuracy: 22.830 %\n",
      "Epoch: 15, Query: 750000, loss: 2.194\n",
      "query: 750000, Accuracy: 22.830 %\n",
      "Epoch: 16, Query: 760000, loss: 2.198\n",
      "query: 760000, Accuracy: 22.790 %\n",
      "Epoch: 16, Query: 770000, loss: 2.209\n",
      "query: 770000, Accuracy: 22.820 %\n",
      "Epoch: 16, Query: 780000, loss: 2.189\n",
      "query: 780000, Accuracy: 22.840 %\n",
      "Epoch: 16, Query: 790000, loss: 2.180\n",
      "query: 790000, Accuracy: 22.820 %\n",
      "Epoch: 16, Query: 800000, loss: 2.179\n",
      "query: 800000, Accuracy: 22.830 %\n",
      "Epoch: 17, Query: 810000, loss: 2.186\n",
      "query: 810000, Accuracy: 22.820 %\n",
      "Epoch: 17, Query: 820000, loss: 2.206\n",
      "query: 820000, Accuracy: 22.830 %\n",
      "Epoch: 17, Query: 830000, loss: 2.189\n",
      "query: 830000, Accuracy: 22.830 %\n",
      "Epoch: 17, Query: 840000, loss: 2.191\n",
      "query: 840000, Accuracy: 22.800 %\n",
      "Epoch: 17, Query: 850000, loss: 2.191\n",
      "query: 850000, Accuracy: 22.830 %\n",
      "Epoch: 18, Query: 860000, loss: 2.184\n",
      "query: 860000, Accuracy: 22.840 %\n",
      "Epoch: 18, Query: 870000, loss: 2.213\n",
      "query: 870000, Accuracy: 22.840 %\n",
      "Epoch: 18, Query: 880000, loss: 2.186\n",
      "query: 880000, Accuracy: 22.830 %\n",
      "Epoch: 18, Query: 890000, loss: 2.199\n",
      "query: 890000, Accuracy: 22.820 %\n",
      "Epoch: 18, Query: 900000, loss: 2.189\n",
      "query: 900000, Accuracy: 22.810 %\n",
      "Epoch: 19, Query: 910000, loss: 2.199\n",
      "query: 910000, Accuracy: 22.820 %\n",
      "Epoch: 19, Query: 920000, loss: 2.191\n",
      "query: 920000, Accuracy: 22.820 %\n",
      "Epoch: 19, Query: 930000, loss: 2.193\n",
      "query: 930000, Accuracy: 22.830 %\n",
      "Epoch: 19, Query: 940000, loss: 2.195\n",
      "query: 940000, Accuracy: 22.830 %\n",
      "Epoch: 19, Query: 950000, loss: 2.177\n",
      "query: 950000, Accuracy: 22.830 %\n",
      "Epoch: 20, Query: 960000, loss: 2.183\n",
      "query: 960000, Accuracy: 22.830 %\n",
      "Epoch: 20, Query: 970000, loss: 2.200\n",
      "query: 970000, Accuracy: 22.830 %\n",
      "Epoch: 20, Query: 980000, loss: 2.193\n",
      "query: 980000, Accuracy: 22.810 %\n",
      "Epoch: 20, Query: 990000, loss: 2.195\n",
      "query: 990000, Accuracy: 22.820 %\n",
      "Epoch: 20, Query: 1000000, loss: 2.187\n",
      "query: 1000000, Accuracy: 22.830 %\n",
      "Epoch: 21, Query: 1010000, loss: 2.186\n",
      "query: 1010000, Accuracy: 22.850 %\n",
      "Epoch: 21, Query: 1020000, loss: 2.193\n",
      "query: 1020000, Accuracy: 22.830 %\n",
      "Epoch: 21, Query: 1030000, loss: 2.191\n",
      "query: 1030000, Accuracy: 22.820 %\n",
      "Epoch: 21, Query: 1040000, loss: 2.186\n",
      "query: 1040000, Accuracy: 22.840 %\n",
      "Epoch: 21, Query: 1050000, loss: 2.203\n",
      "query: 1050000, Accuracy: 22.830 %\n",
      "Epoch: 22, Query: 1060000, loss: 2.191\n",
      "query: 1060000, Accuracy: 22.840 %\n",
      "Epoch: 22, Query: 1070000, loss: 2.204\n",
      "query: 1070000, Accuracy: 22.820 %\n",
      "Epoch: 22, Query: 1080000, loss: 2.174\n",
      "query: 1080000, Accuracy: 22.840 %\n",
      "Epoch: 22, Query: 1090000, loss: 2.196\n",
      "query: 1090000, Accuracy: 22.840 %\n",
      "Epoch: 22, Query: 1100000, loss: 2.193\n",
      "query: 1100000, Accuracy: 22.830 %\n",
      "Epoch: 23, Query: 1110000, loss: 2.190\n",
      "query: 1110000, Accuracy: 22.820 %\n",
      "Epoch: 23, Query: 1120000, loss: 2.200\n",
      "query: 1120000, Accuracy: 22.830 %\n",
      "Epoch: 23, Query: 1130000, loss: 2.194\n",
      "query: 1130000, Accuracy: 22.850 %\n",
      "Epoch: 23, Query: 1140000, loss: 2.191\n",
      "query: 1140000, Accuracy: 22.850 %\n",
      "Epoch: 23, Query: 1150000, loss: 2.189\n",
      "query: 1150000, Accuracy: 22.840 %\n",
      "Epoch: 24, Query: 1160000, loss: 2.194\n",
      "query: 1160000, Accuracy: 22.850 %\n",
      "Epoch: 24, Query: 1170000, loss: 2.197\n",
      "query: 1170000, Accuracy: 22.850 %\n",
      "Epoch: 24, Query: 1180000, loss: 2.197\n",
      "query: 1180000, Accuracy: 22.840 %\n",
      "Epoch: 24, Query: 1190000, loss: 2.190\n",
      "query: 1190000, Accuracy: 22.820 %\n",
      "Epoch: 24, Query: 1200000, loss: 2.185\n",
      "query: 1200000, Accuracy: 22.830 %\n",
      "Epoch: 25, Query: 1210000, loss: 2.200\n",
      "query: 1210000, Accuracy: 22.840 %\n",
      "Epoch: 25, Query: 1220000, loss: 2.190\n",
      "query: 1220000, Accuracy: 22.850 %\n",
      "Epoch: 25, Query: 1230000, loss: 2.192\n",
      "query: 1230000, Accuracy: 22.830 %\n",
      "Epoch: 25, Query: 1240000, loss: 2.188\n",
      "query: 1240000, Accuracy: 22.840 %\n",
      "Epoch: 25, Query: 1250000, loss: 2.180\n",
      "query: 1250000, Accuracy: 22.840 %\n",
      "Epoch: 26, Query: 1260000, loss: 2.202\n",
      "query: 1260000, Accuracy: 22.840 %\n",
      "Epoch: 26, Query: 1270000, loss: 2.208\n",
      "query: 1270000, Accuracy: 22.820 %\n",
      "Epoch: 26, Query: 1280000, loss: 2.183\n",
      "query: 1280000, Accuracy: 22.820 %\n",
      "Epoch: 26, Query: 1290000, loss: 2.183\n",
      "query: 1290000, Accuracy: 22.850 %\n",
      "Epoch: 26, Query: 1300000, loss: 2.184\n",
      "query: 1300000, Accuracy: 22.840 %\n",
      "Epoch: 27, Query: 1310000, loss: 2.191\n",
      "query: 1310000, Accuracy: 22.840 %\n",
      "Epoch: 27, Query: 1320000, loss: 2.195\n",
      "query: 1320000, Accuracy: 22.840 %\n",
      "Epoch: 27, Query: 1330000, loss: 2.195\n",
      "query: 1330000, Accuracy: 22.830 %\n",
      "Epoch: 27, Query: 1340000, loss: 2.212\n",
      "query: 1340000, Accuracy: 22.840 %\n",
      "Epoch: 27, Query: 1350000, loss: 2.186\n",
      "query: 1350000, Accuracy: 22.830 %\n",
      "Epoch: 28, Query: 1360000, loss: 2.192\n",
      "query: 1360000, Accuracy: 22.850 %\n",
      "Epoch: 28, Query: 1370000, loss: 2.205\n",
      "query: 1370000, Accuracy: 22.840 %\n",
      "Epoch: 28, Query: 1380000, loss: 2.185\n",
      "query: 1380000, Accuracy: 22.850 %\n",
      "Epoch: 28, Query: 1390000, loss: 2.187\n",
      "query: 1390000, Accuracy: 22.840 %\n",
      "Epoch: 28, Query: 1400000, loss: 2.199\n",
      "query: 1400000, Accuracy: 22.820 %\n",
      "Epoch: 29, Query: 1410000, loss: 2.196\n",
      "query: 1410000, Accuracy: 22.840 %\n",
      "Epoch: 29, Query: 1420000, loss: 2.198\n",
      "query: 1420000, Accuracy: 22.840 %\n",
      "Epoch: 29, Query: 1430000, loss: 2.196\n",
      "query: 1430000, Accuracy: 22.840 %\n",
      "Epoch: 29, Query: 1440000, loss: 2.184\n",
      "query: 1440000, Accuracy: 22.820 %\n",
      "Epoch: 29, Query: 1450000, loss: 2.190\n",
      "query: 1450000, Accuracy: 22.840 %\n",
      "Epoch: 30, Query: 1460000, loss: 2.185\n",
      "query: 1460000, Accuracy: 22.840 %\n",
      "Epoch: 30, Query: 1470000, loss: 2.195\n",
      "query: 1470000, Accuracy: 22.850 %\n",
      "Epoch: 30, Query: 1480000, loss: 2.187\n",
      "query: 1480000, Accuracy: 22.820 %\n",
      "Epoch: 30, Query: 1490000, loss: 2.184\n",
      "query: 1490000, Accuracy: 22.830 %\n",
      "Epoch: 30, Query: 1500000, loss: 2.199\n",
      "query: 1500000, Accuracy: 22.820 %\n",
      "Epoch: 31, Query: 1510000, loss: 2.189\n",
      "query: 1510000, Accuracy: 22.820 %\n",
      "Epoch: 31, Query: 1520000, loss: 2.190\n",
      "query: 1520000, Accuracy: 22.820 %\n",
      "Epoch: 31, Query: 1530000, loss: 2.194\n",
      "query: 1530000, Accuracy: 22.850 %\n",
      "Epoch: 31, Query: 1540000, loss: 2.196\n",
      "query: 1540000, Accuracy: 22.840 %\n",
      "Epoch: 31, Query: 1550000, loss: 2.183\n",
      "query: 1550000, Accuracy: 22.830 %\n",
      "Epoch: 32, Query: 1560000, loss: 2.176\n",
      "query: 1560000, Accuracy: 22.830 %\n",
      "Epoch: 32, Query: 1570000, loss: 2.198\n",
      "query: 1570000, Accuracy: 22.850 %\n",
      "Epoch: 32, Query: 1580000, loss: 2.190\n",
      "query: 1580000, Accuracy: 22.830 %\n",
      "Epoch: 32, Query: 1590000, loss: 2.184\n",
      "query: 1590000, Accuracy: 22.840 %\n",
      "Epoch: 32, Query: 1600000, loss: 2.193\n",
      "query: 1600000, Accuracy: 22.840 %\n",
      "Epoch: 33, Query: 1610000, loss: 2.186\n",
      "query: 1610000, Accuracy: 22.810 %\n",
      "Epoch: 33, Query: 1620000, loss: 2.197\n",
      "query: 1620000, Accuracy: 22.840 %\n",
      "Epoch: 33, Query: 1630000, loss: 2.178\n",
      "query: 1630000, Accuracy: 22.840 %\n",
      "Epoch: 33, Query: 1640000, loss: 2.191\n",
      "query: 1640000, Accuracy: 22.810 %\n",
      "Epoch: 33, Query: 1650000, loss: 2.205\n",
      "query: 1650000, Accuracy: 22.820 %\n",
      "Epoch: 34, Query: 1660000, loss: 2.203\n",
      "query: 1660000, Accuracy: 22.830 %\n",
      "Epoch: 34, Query: 1670000, loss: 2.196\n",
      "query: 1670000, Accuracy: 22.850 %\n",
      "Epoch: 34, Query: 1680000, loss: 2.178\n",
      "query: 1680000, Accuracy: 22.820 %\n",
      "Epoch: 34, Query: 1690000, loss: 2.190\n",
      "query: 1690000, Accuracy: 22.850 %\n",
      "Epoch: 34, Query: 1700000, loss: 2.207\n",
      "query: 1700000, Accuracy: 22.850 %\n",
      "Epoch: 35, Query: 1710000, loss: 2.188\n",
      "query: 1710000, Accuracy: 22.820 %\n",
      "Epoch: 35, Query: 1720000, loss: 2.198\n",
      "query: 1720000, Accuracy: 22.840 %\n",
      "Epoch: 35, Query: 1730000, loss: 2.188\n",
      "query: 1730000, Accuracy: 22.830 %\n",
      "Epoch: 35, Query: 1740000, loss: 2.201\n",
      "query: 1740000, Accuracy: 22.840 %\n",
      "Epoch: 35, Query: 1750000, loss: 2.199\n",
      "query: 1750000, Accuracy: 22.830 %\n",
      "Epoch: 36, Query: 1760000, loss: 2.212\n",
      "query: 1760000, Accuracy: 22.830 %\n",
      "Epoch: 36, Query: 1770000, loss: 2.190\n",
      "query: 1770000, Accuracy: 22.830 %\n",
      "Epoch: 36, Query: 1780000, loss: 2.191\n",
      "query: 1780000, Accuracy: 22.850 %\n",
      "Epoch: 36, Query: 1790000, loss: 2.195\n",
      "query: 1790000, Accuracy: 22.840 %\n",
      "Epoch: 36, Query: 1800000, loss: 2.198\n",
      "query: 1800000, Accuracy: 22.840 %\n",
      "Epoch: 37, Query: 1810000, loss: 2.196\n",
      "query: 1810000, Accuracy: 22.850 %\n",
      "Epoch: 37, Query: 1820000, loss: 2.194\n",
      "query: 1820000, Accuracy: 22.810 %\n",
      "Epoch: 37, Query: 1830000, loss: 2.214\n",
      "query: 1830000, Accuracy: 22.830 %\n",
      "Epoch: 37, Query: 1840000, loss: 2.191\n",
      "query: 1840000, Accuracy: 22.860 %\n",
      "Epoch: 37, Query: 1850000, loss: 2.183\n",
      "query: 1850000, Accuracy: 22.830 %\n",
      "Epoch: 38, Query: 1860000, loss: 2.199\n",
      "query: 1860000, Accuracy: 22.830 %\n",
      "Epoch: 38, Query: 1870000, loss: 2.194\n",
      "query: 1870000, Accuracy: 22.850 %\n",
      "Epoch: 38, Query: 1880000, loss: 2.179\n",
      "query: 1880000, Accuracy: 22.840 %\n",
      "Epoch: 38, Query: 1890000, loss: 2.186\n",
      "query: 1890000, Accuracy: 22.830 %\n",
      "Epoch: 38, Query: 1900000, loss: 2.192\n",
      "query: 1900000, Accuracy: 22.810 %\n",
      "Epoch: 39, Query: 1910000, loss: 2.195\n",
      "query: 1910000, Accuracy: 22.830 %\n",
      "Epoch: 39, Query: 1920000, loss: 2.206\n",
      "query: 1920000, Accuracy: 22.830 %\n",
      "Epoch: 39, Query: 1930000, loss: 2.186\n",
      "query: 1930000, Accuracy: 22.840 %\n",
      "Epoch: 39, Query: 1940000, loss: 2.201\n",
      "query: 1940000, Accuracy: 22.820 %\n",
      "Epoch: 39, Query: 1950000, loss: 2.198\n",
      "query: 1950000, Accuracy: 22.820 %\n",
      "Epoch: 40, Query: 1960000, loss: 2.192\n",
      "query: 1960000, Accuracy: 22.840 %\n",
      "Epoch: 40, Query: 1970000, loss: 2.203\n",
      "query: 1970000, Accuracy: 22.850 %\n",
      "Epoch: 40, Query: 1980000, loss: 2.195\n",
      "query: 1980000, Accuracy: 22.840 %\n",
      "Epoch: 40, Query: 1990000, loss: 2.195\n",
      "query: 1990000, Accuracy: 22.850 %\n",
      "Epoch: 40, Query: 2000000, loss: 2.188\n",
      "query: 2000000, Accuracy: 22.850 %\n",
      "Epoch: 41, Query: 2010000, loss: 2.198\n",
      "query: 2010000, Accuracy: 22.840 %\n",
      "Epoch: 41, Query: 2020000, loss: 2.187\n",
      "query: 2020000, Accuracy: 22.860 %\n",
      "Epoch: 41, Query: 2030000, loss: 2.200\n",
      "query: 2030000, Accuracy: 22.850 %\n",
      "Epoch: 41, Query: 2040000, loss: 2.188\n",
      "query: 2040000, Accuracy: 22.860 %\n",
      "Epoch: 41, Query: 2050000, loss: 2.202\n",
      "query: 2050000, Accuracy: 22.840 %\n",
      "Epoch: 42, Query: 2060000, loss: 2.192\n",
      "query: 2060000, Accuracy: 22.840 %\n",
      "Epoch: 42, Query: 2070000, loss: 2.191\n",
      "query: 2070000, Accuracy: 22.830 %\n",
      "Epoch: 42, Query: 2080000, loss: 2.198\n",
      "query: 2080000, Accuracy: 22.840 %\n",
      "Epoch: 42, Query: 2090000, loss: 2.190\n",
      "query: 2090000, Accuracy: 22.840 %\n",
      "Epoch: 42, Query: 2100000, loss: 2.182\n",
      "query: 2100000, Accuracy: 22.840 %\n",
      "Epoch: 43, Query: 2110000, loss: 2.206\n",
      "query: 2110000, Accuracy: 22.850 %\n",
      "Epoch: 43, Query: 2120000, loss: 2.190\n",
      "query: 2120000, Accuracy: 22.840 %\n",
      "Epoch: 43, Query: 2130000, loss: 2.193\n",
      "query: 2130000, Accuracy: 22.830 %\n",
      "Epoch: 43, Query: 2140000, loss: 2.204\n",
      "query: 2140000, Accuracy: 22.830 %\n",
      "Epoch: 43, Query: 2150000, loss: 2.188\n",
      "query: 2150000, Accuracy: 22.810 %\n",
      "Epoch: 44, Query: 2160000, loss: 2.189\n",
      "query: 2160000, Accuracy: 22.850 %\n",
      "Epoch: 44, Query: 2170000, loss: 2.192\n",
      "query: 2170000, Accuracy: 22.850 %\n",
      "Epoch: 44, Query: 2180000, loss: 2.184\n",
      "query: 2180000, Accuracy: 22.830 %\n",
      "Epoch: 44, Query: 2190000, loss: 2.179\n",
      "query: 2190000, Accuracy: 22.830 %\n",
      "Epoch: 44, Query: 2200000, loss: 2.187\n",
      "query: 2200000, Accuracy: 22.860 %\n",
      "Epoch: 45, Query: 2210000, loss: 2.210\n",
      "query: 2210000, Accuracy: 22.830 %\n",
      "Epoch: 45, Query: 2220000, loss: 2.187\n",
      "query: 2220000, Accuracy: 22.840 %\n",
      "Epoch: 45, Query: 2230000, loss: 2.191\n",
      "query: 2230000, Accuracy: 22.840 %\n",
      "Epoch: 45, Query: 2240000, loss: 2.188\n",
      "query: 2240000, Accuracy: 22.830 %\n",
      "Epoch: 45, Query: 2250000, loss: 2.192\n",
      "query: 2250000, Accuracy: 22.860 %\n",
      "Epoch: 46, Query: 2260000, loss: 2.190\n",
      "query: 2260000, Accuracy: 22.840 %\n",
      "Epoch: 46, Query: 2270000, loss: 2.193\n",
      "query: 2270000, Accuracy: 22.840 %\n",
      "Epoch: 46, Query: 2280000, loss: 2.183\n",
      "query: 2280000, Accuracy: 22.820 %\n",
      "Epoch: 46, Query: 2290000, loss: 2.181\n",
      "query: 2290000, Accuracy: 22.840 %\n",
      "Epoch: 46, Query: 2300000, loss: 2.186\n",
      "query: 2300000, Accuracy: 22.860 %\n",
      "Epoch: 47, Query: 2310000, loss: 2.175\n",
      "query: 2310000, Accuracy: 22.850 %\n",
      "Epoch: 47, Query: 2320000, loss: 2.200\n",
      "query: 2320000, Accuracy: 22.850 %\n",
      "Epoch: 47, Query: 2330000, loss: 2.196\n",
      "query: 2330000, Accuracy: 22.830 %\n",
      "Epoch: 47, Query: 2340000, loss: 2.203\n",
      "query: 2340000, Accuracy: 22.830 %\n",
      "Epoch: 47, Query: 2350000, loss: 2.199\n",
      "query: 2350000, Accuracy: 22.850 %\n",
      "Epoch: 48, Query: 2360000, loss: 2.190\n",
      "query: 2360000, Accuracy: 22.830 %\n",
      "Epoch: 48, Query: 2370000, loss: 2.188\n",
      "query: 2370000, Accuracy: 22.840 %\n",
      "Epoch: 48, Query: 2380000, loss: 2.191\n",
      "query: 2380000, Accuracy: 22.830 %\n",
      "Epoch: 48, Query: 2390000, loss: 2.200\n",
      "query: 2390000, Accuracy: 22.830 %\n",
      "Epoch: 48, Query: 2400000, loss: 2.191\n",
      "query: 2400000, Accuracy: 22.840 %\n",
      "Epoch: 49, Query: 2410000, loss: 2.197\n",
      "query: 2410000, Accuracy: 22.830 %\n",
      "Epoch: 49, Query: 2420000, loss: 2.190\n",
      "query: 2420000, Accuracy: 22.820 %\n",
      "Epoch: 49, Query: 2430000, loss: 2.196\n",
      "query: 2430000, Accuracy: 22.830 %\n",
      "Epoch: 49, Query: 2440000, loss: 2.202\n",
      "query: 2440000, Accuracy: 22.820 %\n",
      "Epoch: 49, Query: 2450000, loss: 2.189\n",
      "query: 2450000, Accuracy: 22.830 %\n",
      "Epoch: 50, Query: 2460000, loss: 2.197\n",
      "query: 2460000, Accuracy: 22.830 %\n",
      "Epoch: 50, Query: 2470000, loss: 2.206\n",
      "query: 2470000, Accuracy: 22.820 %\n",
      "Epoch: 50, Query: 2480000, loss: 2.198\n",
      "query: 2480000, Accuracy: 22.840 %\n",
      "Epoch: 50, Query: 2490000, loss: 2.193\n",
      "query: 2490000, Accuracy: 22.840 %\n",
      "Epoch: 50, Query: 2500000, loss: 2.198\n",
      "query: 2500000, Accuracy: 22.840 %\n",
      "Epoch: 51, Query: 2510000, loss: 2.204\n",
      "query: 2510000, Accuracy: 22.840 %\n",
      "Epoch: 51, Query: 2520000, loss: 2.186\n",
      "query: 2520000, Accuracy: 22.850 %\n",
      "Epoch: 51, Query: 2530000, loss: 2.197\n",
      "query: 2530000, Accuracy: 22.850 %\n",
      "Epoch: 51, Query: 2540000, loss: 2.190\n",
      "query: 2540000, Accuracy: 22.870 %\n",
      "Epoch: 51, Query: 2550000, loss: 2.192\n",
      "query: 2550000, Accuracy: 22.870 %\n",
      "Epoch: 52, Query: 2560000, loss: 2.199\n",
      "query: 2560000, Accuracy: 22.860 %\n",
      "Epoch: 52, Query: 2570000, loss: 2.171\n",
      "query: 2570000, Accuracy: 22.860 %\n",
      "Epoch: 52, Query: 2580000, loss: 2.186\n",
      "query: 2580000, Accuracy: 22.860 %\n",
      "Epoch: 52, Query: 2590000, loss: 2.191\n",
      "query: 2590000, Accuracy: 22.840 %\n",
      "Epoch: 52, Query: 2600000, loss: 2.205\n",
      "query: 2600000, Accuracy: 22.860 %\n",
      "Epoch: 53, Query: 2610000, loss: 2.190\n",
      "query: 2610000, Accuracy: 22.840 %\n",
      "Epoch: 53, Query: 2620000, loss: 2.192\n",
      "query: 2620000, Accuracy: 22.840 %\n",
      "Epoch: 53, Query: 2630000, loss: 2.179\n",
      "query: 2630000, Accuracy: 22.850 %\n",
      "Epoch: 53, Query: 2640000, loss: 2.192\n",
      "query: 2640000, Accuracy: 22.840 %\n",
      "Epoch: 53, Query: 2650000, loss: 2.189\n",
      "query: 2650000, Accuracy: 22.860 %\n",
      "Epoch: 54, Query: 2660000, loss: 2.199\n",
      "query: 2660000, Accuracy: 22.840 %\n",
      "Epoch: 54, Query: 2670000, loss: 2.196\n",
      "query: 2670000, Accuracy: 22.850 %\n",
      "Epoch: 54, Query: 2680000, loss: 2.196\n",
      "query: 2680000, Accuracy: 22.850 %\n",
      "Epoch: 54, Query: 2690000, loss: 2.191\n",
      "query: 2690000, Accuracy: 22.840 %\n",
      "Epoch: 54, Query: 2700000, loss: 2.185\n",
      "query: 2700000, Accuracy: 22.840 %\n",
      "Epoch: 55, Query: 2710000, loss: 2.191\n",
      "query: 2710000, Accuracy: 22.850 %\n",
      "Epoch: 55, Query: 2720000, loss: 2.204\n",
      "query: 2720000, Accuracy: 22.850 %\n",
      "Epoch: 55, Query: 2730000, loss: 2.177\n",
      "query: 2730000, Accuracy: 22.850 %\n",
      "Epoch: 55, Query: 2740000, loss: 2.186\n",
      "query: 2740000, Accuracy: 22.850 %\n",
      "Epoch: 55, Query: 2750000, loss: 2.180\n",
      "query: 2750000, Accuracy: 22.870 %\n",
      "Epoch: 56, Query: 2760000, loss: 2.194\n",
      "query: 2760000, Accuracy: 22.850 %\n",
      "Epoch: 56, Query: 2770000, loss: 2.204\n",
      "query: 2770000, Accuracy: 22.850 %\n",
      "Epoch: 56, Query: 2780000, loss: 2.192\n",
      "query: 2780000, Accuracy: 22.860 %\n",
      "Epoch: 56, Query: 2790000, loss: 2.189\n",
      "query: 2790000, Accuracy: 22.850 %\n",
      "Epoch: 56, Query: 2800000, loss: 2.199\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (query_size, \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total))\n\u001b[1;32m     62\u001b[0m         plot_data\u001b[38;5;241m.\u001b[39mappend([query_size, \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total])\n\u001b[0;32m---> 64\u001b[0m knockoff_attack(train_loader_cifar10, query_to_victim_softmax)\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mknockoff_attack\u001b[0;34m(data_loader, query_to_vitcim)\u001b[0m\n\u001b[1;32m     35\u001b[0m             lr_scheduler\u001b[38;5;241m.\u001b[39mstep(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;66;03m# check accuracy\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m             test_attacker_model(test_loader_cifar10, query_to_vitcim, query_cnt)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m, in \u001b[0;36mtest_attacker_model\u001b[0;34m(data_loader, query_to_victim, query_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     49\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     51\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     52\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mnormalize(tensor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mnormalize(tensor, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd, inplace\u001b[38;5;241m=\u001b[39minplace)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/_functional_tensor.py:925\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 925\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(attacker_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, threshold=0.001)\n",
    "\n",
    "# for plot\n",
    "plot_data = []\n",
    "\n",
    "# attack\n",
    "def knockoff_attack(data_loader, query_to_vitcim):\n",
    "    \"\"\"\n",
    "    This function performs the knockoff attack on the victim model.\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(data_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get victim output\n",
    "            outputs = query_to_vitcim(inputs)\n",
    "\n",
    "            # train attacker model\n",
    "            attacker_outputs = attacker_model(inputs)\n",
    "            loss = criterion(attacker_outputs, torch.argmax(outputs, dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # check accuracy every 10000 queries\n",
    "            if (i + 1) % 10 == 0:\n",
    "                query_cnt = epoch * batch_size * len(data_loader) + (i + 1) * batch_size\n",
    "                print('Epoch: %d, Query: %d, loss: %.3f' % (epoch + 1, query_cnt, loss.item()))\n",
    "                lr_scheduler.step(loss.item())\n",
    "\n",
    "                # check accuracy\n",
    "                test_attacker_model(test_loader_cifar10, query_to_vitcim, query_cnt)\n",
    "\n",
    "    print('Finished Training') \n",
    "\n",
    "def test_attacker_model(data_loader, query_to_victim, query_size):\n",
    "    \"\"\"\n",
    "    This function tests the attacker model.\n",
    "    \"\"\"\n",
    "    attacker_model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = attacker_model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # _, labels = torch.max(query_to_victim(inputs), 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('query: %d, Accuracy: %.3f %%' % (query_size, 100 * correct / total))\n",
    "        plot_data.append([query_size, 100 * correct / total])\n",
    "\n",
    "knockoff_attack(train_loader_cifar10, query_to_victim_softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save plot data \n",
    "np.save('knockoff_attack.npy', plot_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
